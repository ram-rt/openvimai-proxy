# OpenVimAI Proxy

FastAPI-based backend proxy for OpenVimAI.nvim, with support for streaming, caching, telemetry, and secure JWT-based auth.

This backend streams OpenAI completions over SSE and adds a local cache layer, latency tracking, and token cutoff logic. Designed to work with the [OpenVimAI.nvim plugin](https://github.com/ram-rt/openvimai.nvim).


## 🚀 Features

- Streamed `/completion` endpoint (OpenAI-compatible)

- `/telemetry` ingestion for latency & usage stats

- DuckDB-based persistent caching

- JWT-auth for secure plugin access

- Token budgeting: interrupt long completions

- Docker & Compose-ready


## 📦 Requirements

- Python 3.11+

- OpenAI API Key

- Optional: Docker


🧪 Quickstart (local)

1. Clone the repository:
```bash
git clone https://github.com/ram/openvimai-proxy.git
cd openvimai-proxy
```

2. Copy the example environment file:
```bash
cp .env.example .env
```

3. Edit .env and insert your actual keys:
```env
# .env
OPENAI_API_KEY=your-real-openai-key
JWT_SECRET_KEY=your-own-secret
```

4. Start the backend server:
```bash
docker compose up --build -d
```

Once running, the API will be available at http://localhost:8000
You can test it using http://localhost:8000/docs


## 🔐 Security

All endpoints are protected by JWT via the Authorization: Bearer ... header. Tokens are generated by the plugin using the shared secret.


## 📄 License

MIT — see [LICENSE](./LICENSE)
